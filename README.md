# âœï¸ AI Writing Assistant

A privacy-conscious, author-style-preserving AI writing assistant built using **LLaMA 2**, **LoRA fine-tuning**, **FastAPI**, **Qdrant**, and **Retrieval-Augmented Generation (RAG)**. The system can emulate the writing style of a specific author and generate context-aware outputs grounded in a local semantic knowledge base.

---

# ğŸ§  Overview

This application combines a fine-tuned LLaMA 2 model with a RAG pipeline to produce text aligned with a target authors writing style. Fine-tuning is performed using **LoRA (Low-Rank Adaptation)**, and the assistant supports semantic search over a local vector store to inject contextual information into the generation process.

---

## âš™ï¸ Architecture

```
flowchart TD
    A[User Input] --> B[Embed Input Query]
    B --> C[Semantic Search (Qdrant)]
    C --> D[Retrieve Relevant Chunks]
    D --> E[Augment Prompt]
    E --> F[LLM Generation (LLaMA 2 + LoRA)]
    F --> G[Styled Response]
```

---

## ğŸš€ Tech Stack

| Component           | Description                                                    |
|---------------------|----------------------------------------------------------------|
| **LLaMA 2 7B Chat** | Base LLM used for author-style generation (meta-llama/Llama-2-7b-chat-hf) |
| **LoRA (PEFT)**     | Lightweight fine-tuning for efficient personalization          |
| **SentenceTransformers** | Embedding model for semantic vector search (all-MiniLM-L6-v2)  |
| **Qdrant**          | Vector DB for fast retrieval of relevant document chunks       |
| **FastAPI**         | Backend REST API serving the generation and retrieval endpoints |
| **SQLite**          | Lightweight, local relational DB for storing persistent data    |
| **Uvicorn**         | ASGI server used to run the FastAPI app                         |
| **Frontend**        | Placeholder for UI components (to be implemented)               |

---

## ğŸ“‚ Project Structure

```
ai-writing-assistant/
â”œâ”€â”€ backend/  
â”‚   â”œâ”€â”€ db.py               # Database connection and logic  
â”‚   â”œâ”€â”€ main.py             # FastAPI entry point  
â”‚   â”œâ”€â”€ model.py            # LLM model loading and inference  
â”‚   â”œâ”€â”€ rag.py              # RAG pipeline logic (retrieval + augmentation)  
â”‚   â””â”€â”€ schemas.py          # Pydantic models for API I/O  
â”œâ”€â”€ data/  
â”‚   â””â”€â”€ database.db         # SQLite database  
â”œâ”€â”€ fine_tuning/  
â”‚   â”œâ”€â”€ config.py           # Training configuration  
â”‚   â”œâ”€â”€ data/  
â”‚   â”‚   â””â”€â”€ author_snippets.json     # Raw snippets used for fine-tuning  
â”‚   â”œâ”€â”€ dataset.jsonl       # Preprocessed training dataset  
â”‚   â”œâ”€â”€ lora_output/        # LoRA fine-tuned model artifacts  
â”‚   â”œâ”€â”€ test_training_args.py   # Unit test for training config  
â”‚   â””â”€â”€ train_lora.py       # Script to fine-tune LLaMA using PEFT + LoRA  
â”œâ”€â”€ frontend/               # Frontend code (to be implemented)  
â”œâ”€â”€ requirements.txt        # Python dependencies  
â””â”€â”€ README.md               # Project documentation
```
---

## ğŸ”§ Local Setup

**Prerequisites:** Python 3.8+, git, virtualenv.

> **Note:** You must request access to LLaMA 2 from Meta and download it via Hugging Face.

```bash
# 1. Clone the repository
git clone https://github.com/awesniuk/ai-writing-assistant.git
cd ai-writing-assistant

# 2. Create and activate a virtual environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# 3. Install project dependencies
pip install -r requirements.txt

# 4. Install additional dependencies for training/inference
pip install peft bitsandbytes accelerate transformers datasets
pip install --upgrade transformers peft

# 5. Configure Accelerate (for multi-GPU, 4-bit, or performance tuning)
accelerate config

# 6. Run the FastAPI server
uvicorn backend.main:app --reload
```
---

## ğŸ§ª Example Use Cases

- âœï¸ Personalized ghostwriting in the style of an author or brand  
- ğŸ“š AI editorial assistants for authors or journalists  
- ğŸ·ï¸ Consistent brand voice across blogs, emails, and ads  
- ğŸ§  Knowledge-grounded assistant for custom corpora  

---

## ğŸ”¬ Fine-Tuning Notes

Uses LoRA (via Hugging Face PEFT) for efficient fine-tuning on small author-specific datasets.

Training data should be formatted as JSONL with the structure:

```json
{"instruction": "...", "input": "...", "output": "..."}

Fine-tuned model weights are saved in /fine_tuning/lora_output/.
```
---

## ğŸ“„ License

This project is released under the MIT License.
Use of the LLaMA 2 model is subject to Metaâ€™s LLaMA license.
